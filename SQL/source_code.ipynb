{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgeopandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mgpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mzipfile\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import warnings\n",
    "from bs4 import BeautifulSoup\n",
    "from sqlalchemy import create_engine\n",
    "from keplergl import KeplerGl\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_PATH = './data'\n",
    "YELLOW_TAXI_URL = 'https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page'\n",
    "ID_LOOKUP_URL = 'https://d37ci6vzurychx.cloudfront.net/misc/taxi_zones.zip'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Data Preprocessing\n",
    "#### Yellow Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_lookup():\n",
    "    \"\"\"Download the Taxi Zones file and get a lookup for location ID.\n",
    "\n",
    "    Returns:\n",
    "        A dataframe contains location IDs and their coordinates.\n",
    "    \"\"\"\n",
    "    zip_path = DATA_PATH + '/taxi_zones.zip'\n",
    "    file_path = DATA_PATH + '/taxi_zones/'\n",
    "    url_data = requests.get(ID_LOOKUP_URL).content\n",
    "    with open(zip_path, 'wb') as f:\n",
    "        f.write(url_data)\n",
    "    zip_data = zipfile.ZipFile(zip_path, 'r')\n",
    "    for f in zip_data.namelist():\n",
    "        zip_data.extract(f, file_path)\n",
    "    zip_data.close()\n",
    "    id_lookup = gpd.read_file(file_path + '/taxi_zones.shp')\n",
    "    id_lookup = id_lookup.to_crs(4326)  # EPSG:4326\n",
    "    id_lookup['Lon'] = id_lookup['geometry'].map(lambda x: x.centroid.x)\n",
    "    id_lookup['Lat'] = id_lookup['geometry'].map(lambda x: x.centroid.y)\n",
    "    id_lookup = id_lookup[['LocationID', 'Lon', 'Lat']]\n",
    "    return id_lookup\n",
    "\n",
    "def get_taxi_urls():\n",
    "    \"\"\"Get urls of yellow taxi parquet data from 2009-01 to 2015-06.\n",
    "\n",
    "    Returns:\n",
    "        A list of urls.\n",
    "    \"\"\"\n",
    "    rsp = requests.get(YELLOW_TAXI_URL)\n",
    "    page_content = BeautifulSoup(rsp.content, 'lxml')\n",
    "    urls = []\n",
    "    for year_tab in page_content.find_all('div', {'id': re.compile('faq\\d{4}')}):\n",
    "        for month_tab in year_tab.find_all('a', {'title': 'Yellow Taxi Trip Records'}):\n",
    "            url = month_tab.get('href')\n",
    "            if url[-15:-8] >= '2009-01' and url[-15:-8] <= '2015-06':\n",
    "                urls.append(url)\n",
    "    return urls\n",
    "\n",
    "def haversine(theta):\n",
    "    \"\"\"The Haversine function for caculating distance.\n",
    "\n",
    "    Args:\n",
    "        theta: A degree in radians.\n",
    "\n",
    "    Returns:\n",
    "        Value of the Haversine function at theta.\n",
    "    \"\"\"\n",
    "    return math.sin(theta / 2.) ** 2\n",
    "\n",
    "def get_distance(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"Get the distance between two longitude and latitude coordinates.\n",
    "\n",
    "    Args:\n",
    "        lon1: Longitude of the first coordinate.\n",
    "        lat1: Latitude of the first coordinate.\n",
    "        lon2: Longitude of the second coordinate.\n",
    "        lat2: Latitude of the second coordinate.\n",
    "\n",
    "    Returns:\n",
    "        Distance between two longitude and latitude coordinates.\n",
    "    \"\"\"\n",
    "    lon1 = math.radians(lon1)\n",
    "    lat1 = math.radians(lat1)\n",
    "    lon2 = math.radians(lon2)\n",
    "    lat2 = math.radians(lat2)\n",
    "    hs = haversine(lat2 - lat1) + math.cos(lat1) * math.cos(lat2) * haversine(lon2 - lon1)\n",
    "    return 2 * 6371 * math.asin(hs ** 0.5)\n",
    "\n",
    "def preprocess_taxi_data(url, id_lookup):\n",
    "    \"\"\"Preprocess yellow taxi data in a month.\n",
    "\n",
    "    Args:\n",
    "        url: Url for the data.\n",
    "        id_lookup: An dataframe contains location IDs and their coordinate.\n",
    "\n",
    "    Returns:\n",
    "        A yellow taxi trip dataframe after filtering and cleaning.\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(url, engine='fastparquet')\n",
    "    if 'PULocationID' in df.columns:\n",
    "        df = pd.merge(df, id_lookup, left_on='PULocationID', right_on='LocationID')\n",
    "        df = pd.merge(df, id_lookup, left_on='DOLocationID', right_on='LocationID', suffixes=('', '_DO'))\n",
    "        df = df[['tpep_pickup_datetime', 'tip_amount', 'Lon', 'Lat', 'Lon_DO', 'Lat_DO']]\n",
    "    elif 'pickup_longitude' in df.columns:\n",
    "        df = df[['pickup_datetime', 'tip_amount', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']]\n",
    "    else:\n",
    "        df = df[['Trip_Pickup_DateTime', 'Tip_Amt', 'Start_Lon', 'Start_Lat', 'End_Lon', 'End_Lat']]\n",
    "    # keep necessary columns\n",
    "    df.columns = ['pickup_datetime', 'tip_amount', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "    df.dropna(inplace=True)\n",
    "    # filter geographical area\n",
    "    df = df[(df.pickup_latitude.between(40.560445, 40.908524) & (df.dropoff_latitude.between(40.560445, 40.908524)))\n",
    "        & (df.pickup_longitude.between(-74.242330,-73.717047)) & (df.dropoff_longitude.between(-74.242330,-73.717047))]\n",
    "    # sample len(uber) / months rows\n",
    "    df = df.sample(n=200000 // 78, replace=False)\n",
    "    # compute distance between two coordinates\n",
    "    df['straight_distance'] = list(map(get_distance, df['pickup_longitude'], df['pickup_latitude'], df['dropoff_longitude'], df['dropoff_latitude']))\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "def get_and_preprocess_taxi_data():\n",
    "    \"\"\"Download and preprocess all the yellow taxi data.\n",
    "\n",
    "    Returns:\n",
    "        Full yellow taxi trip dataframe after filtering and cleaning.\n",
    "    \"\"\"\n",
    "    id_lookup = get_id_lookup()\n",
    "    urls = get_taxi_urls()\n",
    "    df_list = []\n",
    "    # save tmp files in case of crashing\n",
    "    if not os.path.exists(DATA_PATH + '/tmp'):\n",
    "        os.makedirs(DATA_PATH + '/tmp')\n",
    "    for url in urls:\n",
    "        month = url[-15:-8]\n",
    "        if month == '2009-12':\n",
    "            df = preprocess_taxi_data(url, id_lookup)\n",
    "            df.to_csv(DATA_PATH + '/tmp/' + month + '.csv', index=False)\n",
    "            df_list.append(df)\n",
    "            print(month + ' done.')\n",
    "    full_df = pd.concat(df_list)\n",
    "    full_df['pickup_datetime'] = pd.to_datetime(full_df['pickup_datetime'])\n",
    "    full_df.sort_values(by='pickup_datetime', inplace=True)\n",
    "    full_df.reset_index(drop=True, inplace=True)\n",
    "    full_df.to_csv(DATA_PATH + '/taxi_df.csv', index=False)\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_preprocess_uber_data():\n",
    "    \"\"\"Download and preprocess the uber data.\n",
    "\n",
    "    Returns:\n",
    "        Uber dataframe after filtering and cleaning.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv('./data/uber_rides_sample.csv')\n",
    "    # filter abnormal rows\n",
    "    df = df[(df['passenger_count'] > 0) & (df['passenger_count'] < 10)]\n",
    "    df = df[['pickup_datetime', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']]\n",
    "    df['straight_distance'] = list(map(get_distance, df['pickup_longitude'], df['pickup_latitude'], df['dropoff_longitude'], df['dropoff_latitude']))\n",
    "    df.dropna(inplace=True)\n",
    "    # filter geographical area\n",
    "    df = df[(df.pickup_latitude.between(40.560445, 40.908524) & (df.dropoff_latitude.between(40.560445, 40.908524)))\n",
    "        & (df.pickup_longitude.between(-74.242330,-73.717047)) & (df.dropoff_longitude.between(-74.242330,-73.717047))]\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "    df.sort_values(by='pickup_datetime', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hourly_weather_data(df):\n",
    "    \"\"\"Preprocess weather data in hour intervals.\n",
    "\n",
    "    Args:\n",
    "        df: Raw weather dataframe.\n",
    "\n",
    "    Returns:\n",
    "        Hourly weather data after filtering and cleaning.\n",
    "    \"\"\"\n",
    "    df = df[['DATE', 'HourlyPrecipitation', 'HourlyWindSpeed']].copy()\n",
    "    df['DATE'] = pd.to_datetime(df['DATE']).dt.floor('h')\n",
    "    df['HourlyPrecipitation'] = pd.to_numeric(df['HourlyPrecipitation'], errors='coerce')\n",
    "    df['HourlyWindSpeed'] = pd.to_numeric(df['HourlyWindSpeed'], errors='coerce')\n",
    "    df.drop_duplicates('DATE', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.columns = ['date', 'hourly_precipitation', 'hourly_wind_speed']\n",
    "    return df\n",
    "\n",
    "def get_daily_weather_data(df):\n",
    "    \"\"\"Preprocess weather data in day intervals.\n",
    "\n",
    "    Args:\n",
    "        df: Raw weather dataframe.\n",
    "\n",
    "    Returns:\n",
    "        Daily weather data after filtering and cleaning.\n",
    "    \"\"\"\n",
    "    df = df[['DATE', 'DailyAverageWindSpeed']].copy()\n",
    "    df['DATE'] = pd.to_datetime(df['DATE']).dt.floor('d')\n",
    "    df['DailyAverageWindSpeed'] = pd.to_numeric(df['DailyAverageWindSpeed'], errors='coerce')\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop_duplicates('DATE', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.columns = ['date', 'daily_average_wind_speed']\n",
    "    return df\n",
    "\n",
    "def get_and_preprocess_weather_data():\n",
    "    \"\"\"Download and preprocess the weather data.\n",
    "\n",
    "    Returns:\n",
    "        Hourly weather dataframe and daily weather dataframe.\n",
    "    \"\"\"\n",
    "    hourly = []\n",
    "    daily = []\n",
    "    for f in os.listdir('./data/weather'):\n",
    "        df = pd.read_csv('./data/weather/' + f)\n",
    "        hourly.append(get_hourly_weather_data(df))\n",
    "        daily.append(get_daily_weather_data(df))\n",
    "    hourly_weather_df = pd.concat(hourly)\n",
    "    hourly_weather_df.sort_values(by='date', inplace=True)\n",
    "    hourly_weather_df.reset_index(drop=True, inplace=True)\n",
    "    daily_weather_df = pd.concat(daily)\n",
    "    daily_weather_df.sort_values(by='date', inplace=True)\n",
    "    daily_weather_df.reset_index(drop=True, inplace=True)\n",
    "    return hourly_weather_df, daily_weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
